{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ8ndbQ72ZgT",
        "outputId": "918040bc-86bd-4f61-f36d-2869a6ab620d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0grUPpaY-B5v",
        "outputId": "52af121a-0d67-4d40-b0ad-e37ad3b7f7c9"
      },
      "outputs": [],
      "source": [
        "#LOAD THE DATASET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "path = '/content/drive/MyDrive/DiabetesPrediction/diabetes.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "#PRINT FIRST 5 ROWS OF THE DATASET\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX1Bt9kB-5z5",
        "outputId": "8f440963-eb35-4b86-d951-049fb06a3f29"
      },
      "outputs": [],
      "source": [
        "#PRINT THE INFO OF THE DATASET\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA1bjhjo_A45",
        "outputId": "cb21c1d9-fdf7-4d40-d12a-9cc6f869e85b"
      },
      "outputs": [],
      "source": [
        "#PRINT THE SUMMARY STATISTICS OF THE DATASET\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnpyroh__IyN"
      },
      "outputs": [],
      "source": [
        "#HANDLING MISSING OR ZERO VALUES\n",
        "\n",
        "#REPLACE ZEROS IN BIOLOGICALLY IMPLAUSIBLE COLUMNS\n",
        "zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "#REPLACE 0s WITH NaN AND FILL WITH MEDIAN\n",
        "for col in zero_columns:\n",
        "  df[col] = df[col].replace(0, np.nan)\n",
        "  median = df[col].median()\n",
        "  df[col] = df[col].fillna(median)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ABfs7bcxE1ZC",
        "outputId": "d30278b9-29ea-4d44-a179-b56fd7310126"
      },
      "outputs": [],
      "source": [
        "#VISUALIZATIONS\n",
        "\n",
        "#CLASS DISTRIBUTION\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='Outcome')\n",
        "plt.title('DIabetes Class Distribution (0 = No, 1 = Yes)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "SmwY-3rgAYZ_",
        "outputId": "0fd52372-0eb5-4321-b7b2-445f6641f29e"
      },
      "outputs": [],
      "source": [
        "#FEATURE DISTRIBUTIONS\n",
        "df.hist(figsize=(12,10), bins=20)\n",
        "plt.suptitle(\"Feature Distributions\")\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "xmrU3F2oBCbJ",
        "outputId": "74c276f6-6f86-459d-f6dc-b31f035d3398"
      },
      "outputs": [],
      "source": [
        "#CORRELATION HEATMAP\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buyXWtNmC-Os"
      },
      "outputs": [],
      "source": [
        "#FEATURE SCALING #STANDARDIZATION\n",
        "X = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqJDSYOE57a"
      },
      "source": [
        "Standardization is generally better for structured/tabular data like this one (Pima Indian Diabetes).\n",
        "\n",
        "Normalization (0–1) is more common with image data or bounded inputs (e.g., pixel intensities, audio samples).\n",
        "\n",
        "We applied standardization to the features using StandardScaler so that each feature has a mean of 0 and a standard deviation of 1. This helps improve model convergence and ensures that the network treats all features on a comparable scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsntpJ39E8U0",
        "outputId": "089721fd-fb37-4268-d96c-05b592cb8e76"
      },
      "outputs": [],
      "source": [
        "#DATA SPLITTING\n",
        "\n",
        "#FIRST SPLIT THE DATA INTO TRAINING(70%) AND TEMP (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, stratify=y,  random_state=42)\n",
        "\n",
        "#THEN SPLIT THE TEMP DATA INTO VALIDATION(20%) AND TEST(10%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(\"\\nDataset Split Summary:\")\n",
        "print(f\"Training Set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation Set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUX-E6iPvl3",
        "outputId": "66b906a4-031f-43d8-872b-65d0436e453d"
      },
      "outputs": [],
      "source": [
        "#MODEL DEVELOPMENT\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "#Define the Model\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#Compile the Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Early Stopping to Prevent Overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "#Train the Model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data = (X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=1)\n",
        "\n",
        "base_val_loss_index = np.argmin(history.history['val_loss'])\n",
        "base_val_acc =history.history['val_accuracy'][base_val_loss_index]\n",
        "\n",
        "print(f\"Base Model Early-Stopped Validation Accuracy: {base_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "jpL2PvlMRTvW",
        "outputId": "5a762555-2c23-4e85-befc-5c7f5d48fc7c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "z3x4I6afteTU",
        "outputId": "8a88f74b-3b11-4799-d271-74790dde58b5"
      },
      "outputs": [],
      "source": [
        "# Grid Search Architectures\n",
        "network_configs = [\n",
        "    [32, 16],\n",
        "    [64, 32],\n",
        "    [16, 8],\n",
        "    [32, 16, 8],\n",
        "    [64,32,16,8],\n",
        "    [64, 32, 16],\n",
        "    [128, 64, 32],\n",
        "    [32, 32, 16]\n",
        "]\n",
        "\n",
        "val_accuracies = [base_val_acc]  # start with base model accuracy\n",
        "labels = ['Base Model'] + [str(cfg) for cfg in network_configs]\n",
        "\n",
        "# Track best model automatically\n",
        "best_val_acc = base_val_acc\n",
        "best_config = 'Base Model'\n",
        "\n",
        "for config in network_configs:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(config[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    for units in config[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stop],\n",
        "                        verbose=0)\n",
        "\n",
        "    # Get validation accuracy at best validation loss epoch\n",
        "    best_epoch = np.argmin(history.history['val_loss'])\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Auto-track best architecture\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_config = config\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies, marker='o', linestyle='-', color='blue')\n",
        "plt.xticks(ticks=range(len(val_accuracies)), labels=labels, rotation=45)\n",
        "plt.xlabel('Network Configuration')\n",
        "plt.ylabel('Validation Accuracy at Early Stop')\n",
        "plt.title('Grid Search vs Base Model (Early-Stopped Accuracy)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best architecture selected\n",
        "print(f\"\\n🏆 Best Architecture Selected Automatically: {best_config}\")\n",
        "print(f\"✅ Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtxZaS_NChI3",
        "outputId": "7d985338-b631-45d5-f6f0-8c8b30554129"
      },
      "outputs": [],
      "source": [
        "# Identify the best model based on validation accuracy\n",
        "best_index = np.argmax(val_accuracies)\n",
        "best_config = ['Base Model'] + network_configs\n",
        "best_config = best_config[best_index]\n",
        "best_accuracy = val_accuracies[best_index]\n",
        "\n",
        "print(f\"🏆 Best Architecture: {best_config}\")\n",
        "print(f\"✅ Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "best_architecture = best_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPppo_RBFFl6",
        "outputId": "c214675b-1f77-49cf-b779-11f043535e13"
      },
      "outputs": [],
      "source": [
        "#L2 REGULARIZATION\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# Set L2 regularization factor (lambda)\n",
        "l2_lambda = 0.01  # You can experiment with 0.001 or 0.0001 too\n",
        "\n",
        "# Build model with L2 regularization\n",
        "l2_model = Sequential()\n",
        "l2_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_train.shape[1],),\n",
        "                   kernel_regularizer=l2(l2_lambda)))\n",
        "for units in best_architecture[1:]:\n",
        "    l2_model.add(Dense(units, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
        "l2_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_lambda)))\n",
        "\n",
        "# Compile\n",
        "l2_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stop_l2 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "l2_history = l2_model.fit(X_train, y_train,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          epochs=100,\n",
        "                          batch_size=32,\n",
        "                          callbacks=[early_stop_l2],\n",
        "                          verbose=1)\n",
        "\n",
        "# Get validation accuracy at the best epoch\n",
        "best_epoch_l2 = np.argmin(l2_history.history['val_loss'])\n",
        "best_val_acc_l2 = l2_history.history['val_accuracy'][best_epoch_l2]\n",
        "print(f\"\\n✅ L2-Regularized Model Early-Stopped Validation Accuracy: {best_val_acc_l2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "BkyMbzxuGmjK",
        "outputId": "8761d851-e0b8-4e53-87cf-07b3b702fd72"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get training history\n",
        "loss = l2_history.history['loss']\n",
        "val_loss = l2_history.history['val_loss']\n",
        "acc = l2_history.history['accuracy']\n",
        "val_acc = l2_history.history['val_accuracy']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, label='Training Loss', color='blue')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss', color='orange')\n",
        "plt.axvline(x=best_epoch_l2 + 1, color='red', linestyle='--', label='Early Stop Epoch')\n",
        "plt.title('Loss with L2 Regularization')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc, label='Training Accuracy', color='blue')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy', color='orange')\n",
        "plt.axvline(x=best_epoch_l2 + 1, color='red', linestyle='--', label='Early Stop Epoch')\n",
        "plt.title('Accuracy with L2 Regularization')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "ALrWZwhSJeV5",
        "outputId": "ab0fd65e-e822-42ee-ce3a-e958cf1d69aa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get epoch counts\n",
        "epochs_best = range(1, len(history.history['loss']) + 1)\n",
        "epochs_l2 = range(1, len(l2_history.history['loss']) + 1)\n",
        "\n",
        "# Plot Loss Comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_best, history.history['val_loss'], label='Best Model Val Loss', color='blue', linestyle='-')\n",
        "plt.plot(epochs_best, history.history['loss'], label='Best Model Train Loss', color='blue', linestyle='--')\n",
        "plt.plot(epochs_l2, l2_history.history['val_loss'], label='L2 Model Val Loss', color='orange', linestyle='-')\n",
        "plt.plot(epochs_l2, l2_history.history['loss'], label='L2 Model Train Loss', color='orange', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation and Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_best, history.history['val_accuracy'], label='Best Model Val Accuracy', color='blue', linestyle='-')\n",
        "plt.plot(epochs_best, history.history['accuracy'], label='Best Model Train Accuracy', color='blue', linestyle='--')\n",
        "plt.plot(epochs_l2, l2_history.history['val_accuracy'], label='L2 Model Val Accuracy', color='orange', linestyle='-')\n",
        "plt.plot(epochs_l2, l2_history.history['accuracy'], label='L2 Model Train Accuracy', color='orange', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation and Training Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "r5D559GWQglP",
        "outputId": "86b8ee8a-2365-401a-f6d9-58a2d0948676"
      },
      "outputs": [],
      "source": [
        "#ADDING DROPOUT\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Dropout rates to try\n",
        "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "dropout_accuracies = []\n",
        "dropout_histories = []\n",
        "\n",
        "for rate in dropout_rates:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(rate))\n",
        "\n",
        "    for units in best_architecture[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(rate))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stop],\n",
        "                        verbose=0)\n",
        "\n",
        "    best_epoch = np.argmin(history.history['val_loss'])\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "\n",
        "    dropout_accuracies.append(val_acc)\n",
        "    dropout_histories.append(history)\n",
        "\n",
        "# Find the best dropout rate\n",
        "best_idx = np.argmax(dropout_accuracies)\n",
        "best_dropout_rate = dropout_rates[best_idx]\n",
        "best_val_acc = dropout_accuracies[best_idx]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(dropout_rates, dropout_accuracies, marker='o', linestyle='-', color='purple')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Dropout Rate vs Validation Accuracy (Early-Stopped)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best result\n",
        "print(f\"🏆 Best Dropout Rate: {best_dropout_rate}\")\n",
        "print(f\"✅ Best Validation Accuracy with Dropout: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0JoOE7naISb",
        "outputId": "e856970e-4dd0-448f-9f78-cde17aeb5763"
      },
      "outputs": [],
      "source": [
        "# 1. Combine training and validation sets for final training\n",
        "X_final_train = np.concatenate((X_train, X_val), axis=0)\n",
        "y_final_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "\n",
        "# 2. Build the final model with best architecture and best dropout rate\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_final_train.shape[1],)))\n",
        "final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "# 2. Build the final model with best architecture and best dropout rate\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_final_train.shape[1],)))\n",
        "final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "for units in best_architecture[1:]:\n",
        "    final_model.add(Dense(units, activation='relu'))\n",
        "    final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "final_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 3. Compile the model\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train on combined train + val set with early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = final_model.fit(X_final_train, y_final_train,\n",
        "                          validation_split=0.1,  # small split from combined for early stopping\n",
        "                          epochs=100,\n",
        "                          batch_size=32,\n",
        "                          callbacks=[early_stop],\n",
        "                          verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Vzy_egWiaiyn",
        "outputId": "00ebeead-0fee-4f4d-f1a2-04d66303da94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "tkqaSdsVxQtf",
        "outputId": "8577f44b-11d1-479c-833a-8672f36004ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 5. Evaluate on test set\n",
        "\n",
        "# Predict probabilities and classes on test set\n",
        "y_prob = final_model.predict(X_test).ravel()\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Visualize ROC Curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\", color='blue', linewidth=2)\n",
        "plt.plot([0,1], [0,1], 'k--', label=\"Random Guessing\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate for different classification thresholds.\n",
        "A curve closer to the top-left corner indicates better classification performance.\n",
        "The AUC (Area Under Curve) summarizes the model's overall ability to discriminate between positive and negative classes:\n",
        "- AUC = 1 means perfect classification,\n",
        "- AUC = 0.5 means no better than random guessing.\n",
        "Your model's AUC indicates how well it can distinguish diabetes cases from non-cases across all thresholds.\n",
        "\"\"\")\n",
        "\n",
        "# 6. Save the final trained model\n",
        "final_model.save('final_best_model_with_dropout.h5')\n",
        "print(\"✅ Final trained model saved as 'final_best_model_with_dropout.h5'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7yiMjSL9lyx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
