{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ8ndbQ72ZgT",
        "outputId": "918040bc-86bd-4f61-f36d-2869a6ab620d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0grUPpaY-B5v",
        "outputId": "52af121a-0d67-4d40-b0ad-e37ad3b7f7c9"
      },
      "outputs": [],
      "source": [
        "#LOAD THE DATASET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "path = '/content/drive/MyDrive/DiabetesPrediction/diabetes.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "#PRINT FIRST 5 ROWS OF THE DATASET\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX1Bt9kB-5z5",
        "outputId": "8f440963-eb35-4b86-d951-049fb06a3f29"
      },
      "outputs": [],
      "source": [
        "#PRINT THE INFO OF THE DATASET\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA1bjhjo_A45",
        "outputId": "cb21c1d9-fdf7-4d40-d12a-9cc6f869e85b"
      },
      "outputs": [],
      "source": [
        "#PRINT THE SUMMARY STATISTICS OF THE DATASET\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnpyroh__IyN"
      },
      "outputs": [],
      "source": [
        "#HANDLING MISSING OR ZERO VALUES\n",
        "\n",
        "#REPLACE ZEROS IN BIOLOGICALLY IMPLAUSIBLE COLUMNS\n",
        "zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "#REPLACE 0s WITH NaN AND FILL WITH MEDIAN\n",
        "for col in zero_columns:\n",
        "  df[col] = df[col].replace(0, np.nan)\n",
        "  median = df[col].median()\n",
        "  df[col] = df[col].fillna(median)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ABfs7bcxE1ZC",
        "outputId": "d30278b9-29ea-4d44-a179-b56fd7310126"
      },
      "outputs": [],
      "source": [
        "#VISUALIZATIONS\n",
        "\n",
        "#CLASS DISTRIBUTION\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='Outcome')\n",
        "plt.title('DIabetes Class Distribution (0 = No, 1 = Yes)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "SmwY-3rgAYZ_",
        "outputId": "0fd52372-0eb5-4321-b7b2-445f6641f29e"
      },
      "outputs": [],
      "source": [
        "#FEATURE DISTRIBUTIONS\n",
        "df.hist(figsize=(12,10), bins=20)\n",
        "plt.suptitle(\"Feature Distributions\")\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "xmrU3F2oBCbJ",
        "outputId": "74c276f6-6f86-459d-f6dc-b31f035d3398"
      },
      "outputs": [],
      "source": [
        "#CORRELATION HEATMAP\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buyXWtNmC-Os"
      },
      "outputs": [],
      "source": [
        "#FEATURE SCALING #STANDARDIZATION\n",
        "X = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqJDSYOE57a"
      },
      "source": [
        "Standardization is generally better for structured/tabular data like this one (Pima Indian Diabetes).\n",
        "\n",
        "Normalization (0â€“1) is more common with image data or bounded inputs (e.g., pixel intensities, audio samples).\n",
        "\n",
        "We applied standardization to the features using StandardScaler so that each feature has a mean of 0 and a standard deviation of 1. This helps improve model convergence and ensures that the network treats all features on a comparable scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsntpJ39E8U0",
        "outputId": "089721fd-fb37-4268-d96c-05b592cb8e76"
      },
      "outputs": [],
      "source": [
        "#DATA SPLITTING\n",
        "\n",
        "#FIRST SPLIT THE DATA INTO TRAINING(70%) AND TEMP (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, stratify=y,  random_state=42)\n",
        "\n",
        "#THEN SPLIT THE TEMP DATA INTO VALIDATION(20%) AND TEST(10%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(\"\\nDataset Split Summary:\")\n",
        "print(f\"Training Set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation Set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUX-E6iPvl3",
        "outputId": "66b906a4-031f-43d8-872b-65d0436e453d"
      },
      "outputs": [],
      "source": [
        "#MODEL DEVELOPMENT\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "#Define the Model\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#Compile the Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Early Stopping to Prevent Overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "#Train the Model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data = (X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=1)\n",
        "\n",
        "base_val_loss_index = np.argmin(history.history['val_loss'])\n",
        "base_val_acc =history.history['val_accuracy'][base_val_loss_index]\n",
        "\n",
        "print(f\"Base Model Early-Stopped Validation Accuracy: {base_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "jpL2PvlMRTvW",
        "outputId": "5a762555-2c23-4e85-befc-5c7f5d48fc7c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "z3x4I6afteTU",
        "outputId": "8a88f74b-3b11-4799-d271-74790dde58b5"
      },
      "outputs": [],
      "source": [
        "# Grid Search Architectures\n",
        "network_configs = [\n",
        "    [32, 16],\n",
        "    [64, 32],\n",
        "    [16, 8],\n",
        "    [32, 16, 8],\n",
        "    [64,32,16,8],\n",
        "    [64, 32, 16],\n",
        "    [128, 64, 32],\n",
        "    [32, 32, 16]\n",
        "]\n",
        "\n",
        "val_accuracies = [base_val_acc]  # start with base model accuracy\n",
        "labels = ['Base Model'] + [str(cfg) for cfg in network_configs]\n",
        "\n",
        "# Track best model automatically\n",
        "best_val_acc = base_val_acc\n",
        "best_config = 'Base Model'\n",
        "\n",
        "for config in network_configs:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(config[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    for units in config[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stop],\n",
        "                        verbose=0)\n",
        "\n",
        "    # Get validation accuracy at best validation loss epoch\n",
        "    best_epoch = np.argmin(history.history['val_loss'])\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Auto-track best architecture\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_config = config\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies, marker='o', linestyle='-', color='blue')\n",
        "plt.xticks(ticks=range(len(val_accuracies)), labels=labels, rotation=45)\n",
        "plt.xlabel('Network Configuration')\n",
        "plt.ylabel('Validation Accuracy at Early Stop')\n",
        "plt.title('Grid Search vs Base Model (Early-Stopped Accuracy)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best architecture selected\n",
        "print(f\"\\nðŸ† Best Architecture Selected Automatically: {best_config}\")\n",
        "print(f\"âœ… Best Validation Accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtxZaS_NChI3",
        "outputId": "7d985338-b631-45d5-f6f0-8c8b30554129"
      },
      "outputs": [],
      "source": [
        "# Identify the best model based on validation accuracy\n",
        "best_index = np.argmax(val_accuracies)\n",
        "best_config = ['Base Model'] + network_configs\n",
        "best_config = best_config[best_index]\n",
        "best_accuracy = val_accuracies[best_index]\n",
        "\n",
        "print(f\"ðŸ† Best Architecture: {best_config}\")\n",
        "print(f\"âœ… Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "best_architecture = best_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPppo_RBFFl6",
        "outputId": "c214675b-1f77-49cf-b779-11f043535e13"
      },
      "outputs": [],
      "source": [
        "#L2 REGULARIZATION\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# Set L2 regularization factor (lambda)\n",
        "l2_lambda = 0.01  # You can experiment with 0.001 or 0.0001 too\n",
        "\n",
        "# Build model with L2 regularization\n",
        "l2_model = Sequential()\n",
        "l2_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_train.shape[1],),\n",
        "                   kernel_regularizer=l2(l2_lambda)))\n",
        "for units in best_architecture[1:]:\n",
        "    l2_model.add(Dense(units, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
        "l2_model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_lambda)))\n",
        "\n",
        "# Compile\n",
        "l2_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stop_l2 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "l2_history = l2_model.fit(X_train, y_train,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          epochs=100,\n",
        "                          batch_size=32,\n",
        "                          callbacks=[early_stop_l2],\n",
        "                          verbose=1)\n",
        "\n",
        "# Get validation accuracy at the best epoch\n",
        "best_epoch_l2 = np.argmin(l2_history.history['val_loss'])\n",
        "best_val_acc_l2 = l2_history.history['val_accuracy'][best_epoch_l2]\n",
        "print(f\"\\nâœ… L2-Regularized Model Early-Stopped Validation Accuracy: {best_val_acc_l2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "BkyMbzxuGmjK",
        "outputId": "8761d851-e0b8-4e53-87cf-07b3b702fd72"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get training history\n",
        "loss = l2_history.history['loss']\n",
        "val_loss = l2_history.history['val_loss']\n",
        "acc = l2_history.history['accuracy']\n",
        "val_acc = l2_history.history['val_accuracy']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, label='Training Loss', color='blue')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss', color='orange')\n",
        "plt.axvline(x=best_epoch_l2 + 1, color='red', linestyle='--', label='Early Stop Epoch')\n",
        "plt.title('Loss with L2 Regularization')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc, label='Training Accuracy', color='blue')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy', color='orange')\n",
        "plt.axvline(x=best_epoch_l2 + 1, color='red', linestyle='--', label='Early Stop Epoch')\n",
        "plt.title('Accuracy with L2 Regularization')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "ALrWZwhSJeV5",
        "outputId": "ab0fd65e-e822-42ee-ce3a-e958cf1d69aa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get epoch counts\n",
        "epochs_best = range(1, len(history.history['loss']) + 1)\n",
        "epochs_l2 = range(1, len(l2_history.history['loss']) + 1)\n",
        "\n",
        "# Plot Loss Comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_best, history.history['val_loss'], label='Best Model Val Loss', color='blue', linestyle='-')\n",
        "plt.plot(epochs_best, history.history['loss'], label='Best Model Train Loss', color='blue', linestyle='--')\n",
        "plt.plot(epochs_l2, l2_history.history['val_loss'], label='L2 Model Val Loss', color='orange', linestyle='-')\n",
        "plt.plot(epochs_l2, l2_history.history['loss'], label='L2 Model Train Loss', color='orange', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation and Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_best, history.history['val_accuracy'], label='Best Model Val Accuracy', color='blue', linestyle='-')\n",
        "plt.plot(epochs_best, history.history['accuracy'], label='Best Model Train Accuracy', color='blue', linestyle='--')\n",
        "plt.plot(epochs_l2, l2_history.history['val_accuracy'], label='L2 Model Val Accuracy', color='orange', linestyle='-')\n",
        "plt.plot(epochs_l2, l2_history.history['accuracy'], label='L2 Model Train Accuracy', color='orange', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation and Training Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "r5D559GWQglP",
        "outputId": "86b8ee8a-2365-401a-f6d9-58a2d0948676"
      },
      "outputs": [],
      "source": [
        "#ADDING DROPOUT\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Dropout rates to try\n",
        "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "dropout_accuracies = []\n",
        "dropout_histories = []\n",
        "\n",
        "for rate in dropout_rates:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(rate))\n",
        "\n",
        "    for units in best_architecture[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(rate))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stop],\n",
        "                        verbose=0)\n",
        "\n",
        "    best_epoch = np.argmin(history.history['val_loss'])\n",
        "    val_acc = history.history['val_accuracy'][best_epoch]\n",
        "\n",
        "    dropout_accuracies.append(val_acc)\n",
        "    dropout_histories.append(history)\n",
        "\n",
        "# Find the best dropout rate\n",
        "best_idx = np.argmax(dropout_accuracies)\n",
        "best_dropout_rate = dropout_rates[best_idx]\n",
        "best_val_acc = dropout_accuracies[best_idx]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(dropout_rates, dropout_accuracies, marker='o', linestyle='-', color='purple')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Dropout Rate vs Validation Accuracy (Early-Stopped)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best result\n",
        "print(f\"ðŸ† Best Dropout Rate: {best_dropout_rate}\")\n",
        "print(f\"âœ… Best Validation Accuracy with Dropout: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0JoOE7naISb",
        "outputId": "e856970e-4dd0-448f-9f78-cde17aeb5763"
      },
      "outputs": [],
      "source": [
        "# 1. Combine training and validation sets for final training\n",
        "X_final_train = np.concatenate((X_train, X_val), axis=0)\n",
        "y_final_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "\n",
        "# 2. Build the final model with best architecture and best dropout rate\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_final_train.shape[1],)))\n",
        "final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "# 2. Build the final model with best architecture and best dropout rate\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(best_architecture[0], activation='relu', input_shape=(X_final_train.shape[1],)))\n",
        "final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "for units in best_architecture[1:]:\n",
        "    final_model.add(Dense(units, activation='relu'))\n",
        "    final_model.add(Dropout(best_dropout_rate))\n",
        "\n",
        "final_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 3. Compile the model\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train on combined train + val set with early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = final_model.fit(X_final_train, y_final_train,\n",
        "                          validation_split=0.1,  # small split from combined for early stopping\n",
        "                          epochs=100,\n",
        "                          batch_size=32,\n",
        "                          callbacks=[early_stop],\n",
        "                          verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Vzy_egWiaiyn",
        "outputId": "00ebeead-0fee-4f4d-f1a2-04d66303da94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "tkqaSdsVxQtf",
        "outputId": "8577f44b-11d1-479c-833a-8672f36004ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 5. Evaluate on test set\n",
        "\n",
        "# Predict probabilities and classes on test set\n",
        "y_prob = final_model.predict(X_test).ravel()\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Visualize ROC Curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\", color='blue', linewidth=2)\n",
        "plt.plot([0,1], [0,1], 'k--', label=\"Random Guessing\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate for different classification thresholds.\n",
        "A curve closer to the top-left corner indicates better classification performance.\n",
        "The AUC (Area Under Curve) summarizes the model's overall ability to discriminate between positive and negative classes:\n",
        "- AUC = 1 means perfect classification,\n",
        "- AUC = 0.5 means no better than random guessing.\n",
        "Your model's AUC indicates how well it can distinguish diabetes cases from non-cases across all thresholds.\n",
        "\"\"\")\n",
        "\n",
        "# 6. Save the final trained model\n",
        "final_model.save('final_best_model_with_dropout.h5')\n",
        "print(\"âœ… Final trained model saved as 'final_best_model_with_dropout.h5'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7yiMjSL9lyx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
